"""Twitter Archiveã‹ã‚‰XæŠ•ç¨¿ã‚’Pineconeã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ.

Twitterè¨­å®šã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ZIPã® tweets.js ã‚’èª­ã¿è¾¼ã¿ã€
#ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ é–¢é€£ã®æŠ•ç¨¿ã‚’Pineconeã«ä¸€æ‹¬ç™»éŒ²ã™ã‚‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    # ZIPã‚’è§£å‡å¾Œã€tweets.js ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
    python scripts/import_tweets_from_archive.py path/to/tweets.js

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿ï¼ˆPineconeã«ä¿å­˜ã—ãªã„ï¼‰
    python scripts/import_tweets_from_archive.py path/to/tweets.js --preview

    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’å¤‰æ›´
    python scripts/import_tweets_from_archive.py path/to/tweets.js --hashtag "#BTC"
"""

import argparse
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.vector_db.pinecone_client import PineconeClient

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# åŒæœŸçŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«
SYNC_STATE_FILE = project_root / "data" / "tweet_sync_state.json"


def load_tweets_js(file_path: str) -> list[dict]:
    """tweets.js ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€.

    tweets.js ã¯ä»¥ä¸‹ã®å½¢å¼:
    window.YTD.tweets.part0 = [ {...}, {...}, ... ]

    Args:
        file_path: tweets.js ã®ãƒ‘ã‚¹

    Returns:
        ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # "window.YTD.tweets.part0 = " ã‚’é™¤å»ã—ã¦JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
    # è¤‡æ•°ã®ãƒ‘ãƒ¼ãƒˆãŒã‚ã‚‹å ´åˆã‚‚å¯¾å¿œ
    json_match = re.search(r"=\s*(\[.*\])\s*$", content, re.DOTALL)
    if not json_match:
        raise ValueError("tweets.js ã®å½¢å¼ãŒä¸æ­£ã§ã™")

    tweets_data = json.loads(json_match.group(1))
    return tweets_data


def parse_tweet(tweet_data: dict) -> dict:
    """ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦çµ±ä¸€å½¢å¼ã«å¤‰æ›.

    Args:
        tweet_data: ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿

    Returns:
        çµ±ä¸€å½¢å¼ã®ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
    """
    tweet = tweet_data.get("tweet", tweet_data)

    # created_at ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆä¾‹: "Sat Nov 30 12:34:56 +0000 2025"ï¼‰
    created_at_str = tweet.get("created_at", "")
    try:
        created_at = datetime.strptime(created_at_str, "%a %b %d %H:%M:%S %z %Y")
    except ValueError:
        # åˆ¥ã®å½¢å¼ã‚’è©¦ã™
        try:
            created_at = datetime.fromisoformat(created_at_str.replace("Z", "+00:00"))
        except ValueError:
            created_at = datetime.now()

    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’æŠ½å‡º
    hashtags = []
    entities = tweet.get("entities", {})
    if "hashtags" in entities:
        hashtags = [ht.get("text", "") for ht in entities["hashtags"]]

    return {
        "id": tweet.get("id_str", tweet.get("id", "")),
        "text": tweet.get("full_text", tweet.get("text", "")),
        "created_at": created_at,
        "hashtags": hashtags,
    }


def filter_by_hashtag(
    tweets: list[dict],
    hashtag: str = "#ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³",
    include_btc: bool = True,
) -> list[dict]:
    """ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°.

    Args:
        tweets: ãƒ„ã‚¤ãƒ¼ãƒˆãƒªã‚¹ãƒˆ
        hashtag: å¯¾è±¡ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°
        include_btc: #BTC, #Bitcoin ã‚‚å«ã‚ã‚‹ã‹

    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ„ã‚¤ãƒ¼ãƒˆãƒªã‚¹ãƒˆ
    """
    target_tags = [hashtag.lower().replace("#", "")]
    if include_btc:
        target_tags.extend(["btc", "bitcoin"])

    filtered = []
    for tweet in tweets:
        text_lower = tweet["text"].lower()
        tweet_tags = [tag.lower() for tag in tweet.get("hashtags", [])]

        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        has_target = any(
            tag in tweet_tags or f"#{tag}" in text_lower
            for tag in target_tags
        )

        if has_target:
            filtered.append(tweet)

    return filtered


def update_sync_state(newest_tweet_id: str, oldest_tweet_id: str, count: int) -> None:
    """åŒæœŸçŠ¶æ…‹ã‚’æ›´æ–°.

    Args:
        newest_tweet_id: æœ€æ–°ã®ãƒ„ã‚¤ãƒ¼ãƒˆID
        oldest_tweet_id: æœ€å¤ã®ãƒ„ã‚¤ãƒ¼ãƒˆID
        count: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸä»¶æ•°
    """
    state = {}
    if SYNC_STATE_FILE.exists():
        with open(SYNC_STATE_FILE, "r", encoding="utf-8") as f:
            state = json.load(f)

    # æœ€æ–°IDã‚’æ›´æ–°ï¼ˆæ—¢å­˜ã‚ˆã‚Šæ–°ã—ã„å ´åˆï¼‰
    current_newest = state.get("newest_synced_tweet_id")
    if current_newest is None or int(newest_tweet_id) > int(current_newest):
        state["newest_synced_tweet_id"] = newest_tweet_id

    # æœ€å¤IDã‚’æ›´æ–°ï¼ˆæ—¢å­˜ã‚ˆã‚Šå¤ã„å ´åˆï¼‰
    current_oldest = state.get("oldest_synced_tweet_id")
    if current_oldest is None or int(oldest_tweet_id) < int(current_oldest):
        state["oldest_synced_tweet_id"] = oldest_tweet_id

    # ãã®ä»–ã®çŠ¶æ…‹ã‚’æ›´æ–°
    state["all_historical_collected"] = True  # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†
    state["total_synced_count"] = state.get("total_synced_count", 0) + count
    state["last_sync_date"] = datetime.now().isoformat()
    state["archive_imported"] = True
    state["archive_import_date"] = datetime.now().isoformat()

    # æœˆé–“ã‚«ã‚¦ãƒ³ãƒˆã¯ç¶­æŒï¼ˆAPIçµŒç”±ã§ã¯ãªã„ã®ã§ã‚«ã‚¦ãƒ³ãƒˆã—ãªã„ï¼‰
    if "api_call_month" not in state:
        state["api_call_month"] = datetime.now().strftime("%Y-%m")
    if "monthly_api_calls" not in state:
        state["monthly_api_calls"] = 0

    SYNC_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(SYNC_STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def import_tweets(
    file_path: str,
    hashtag: str = "#ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³",
    include_btc: bool = True,
    preview: bool = False,
    batch_size: int = 50,
) -> dict:
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ.

    Args:
        file_path: tweets.js ã®ãƒ‘ã‚¹
        hashtag: å¯¾è±¡ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°
        include_btc: #BTC, #Bitcoin ã‚‚å«ã‚ã‚‹ã‹
        preview: ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿ï¼ˆä¿å­˜ã—ãªã„ï¼‰
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœ
    """
    logger.info(f"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {file_path}")

    # tweets.js ã‚’èª­ã¿è¾¼ã¿
    try:
        raw_tweets = load_tweets_js(file_path)
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}

    logger.info(f"ğŸ“Š ç·ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {len(raw_tweets)}")

    # ãƒ‘ãƒ¼ã‚¹ã—ã¦çµ±ä¸€å½¢å¼ã«å¤‰æ›
    tweets = [parse_tweet(t) for t in raw_tweets]

    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã§ãƒ•ã‚£ãƒ«ã‚¿
    filtered = filter_by_hashtag(tweets, hashtag, include_btc)
    logger.info(f"ğŸ·ï¸  {hashtag} ã«ä¸€è‡´: {len(filtered)} ä»¶")

    if not filtered:
        return {
            "success": True,
            "total_tweets": len(raw_tweets),
            "filtered_count": 0,
            "imported_count": 0,
            "message": "å¯¾è±¡ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
        }

    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    filtered.sort(key=lambda x: x["created_at"], reverse=True)

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
    if preview:
        print("\n" + "=" * 60)
        print(f"ğŸ“‹ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€æ–°10ä»¶ / å…¨{len(filtered)}ä»¶ï¼‰")
        print("=" * 60)
        for i, tweet in enumerate(filtered[:10], 1):
            date_str = tweet["created_at"].strftime("%Y-%m-%d")
            text_preview = tweet["text"][:50].replace("\n", " ")
            print(f"{i:2}. [{date_str}] {text_preview}...")
        print("=" * 60)

        return {
            "success": True,
            "total_tweets": len(raw_tweets),
            "filtered_count": len(filtered),
            "imported_count": 0,
            "preview": True,
        }

    # Pinecone ã«ä¿å­˜
    pinecone_client = PineconeClient()
    if not pinecone_client.is_configured:
        logger.error("Pinecone client not configured")
        return {"success": False, "error": "Pinecone client not configured"}

    # ãƒãƒƒãƒå‡¦ç†
    tweets_for_pinecone = []
    for tweet in filtered:
        tweets_for_pinecone.append({
            "tweet_id": tweet["id"],
            "text": tweet["text"],
            "created_at": tweet["created_at"],
            "hashtags": tweet.get("hashtags", []),
            "btc_price": None,
        })

    logger.info(f"ğŸ“¤ Pineconeã¸ä¿å­˜ä¸­... ({len(tweets_for_pinecone)} ä»¶)")

    imported_count = 0
    for i in range(0, len(tweets_for_pinecone), batch_size):
        batch = tweets_for_pinecone[i:i + batch_size]
        count = pinecone_client.upsert_tweets_batch(batch)
        imported_count += count
        logger.info(f"  ãƒãƒƒãƒ {i // batch_size + 1}: {count} ä»¶ä¿å­˜")

    # åŒæœŸçŠ¶æ…‹ã‚’æ›´æ–°
    newest_id = filtered[0]["id"]  # æœ€æ–°
    oldest_id = filtered[-1]["id"]  # æœ€å¤
    update_sync_state(newest_id, oldest_id, imported_count)

    total_in_db = pinecone_client.get_tweet_count()

    return {
        "success": True,
        "total_tweets": len(raw_tweets),
        "filtered_count": len(filtered),
        "imported_count": imported_count,
        "total_in_db": total_in_db,
        "newest_id": newest_id,
        "oldest_id": oldest_id,
        "date_range": {
            "newest": filtered[0]["created_at"].strftime("%Y-%m-%d"),
            "oldest": filtered[-1]["created_at"].strftime("%Y-%m-%d"),
        },
    }


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†."""
    parser = argparse.ArgumentParser(
        description="Twitter Archiveã‹ã‚‰XæŠ•ç¨¿ã‚’Pineconeã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
    )
    parser.add_argument(
        "file_path",
        type=str,
        help="tweets.js ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--hashtag",
        type=str,
        default="#ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³",
        help="å¯¾è±¡ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: #ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ï¼‰",
    )
    parser.add_argument(
        "--no-btc",
        action="store_true",
        help="#BTC, #Bitcoin ã‚’é™¤å¤–",
    )
    parser.add_argument(
        "--preview",
        action="store_true",
        help="ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿ï¼ˆPineconeã«ä¿å­˜ã—ãªã„ï¼‰",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰",
    )

    args = parser.parse_args()

    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    if not Path(args.file_path).exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.file_path}")
        sys.exit(1)

    print("\n" + "=" * 60)
    print("ğŸ“¥ Twitter Archive ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
    print("=" * 60)
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {args.file_path}")
    print(f"ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: {args.hashtag}")
    print(f"ãƒ¢ãƒ¼ãƒ‰: {'ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼' if args.preview else 'æœ¬ç•ªã‚¤ãƒ³ãƒãƒ¼ãƒˆ'}")
    print("=" * 60 + "\n")

    result = import_tweets(
        file_path=args.file_path,
        hashtag=args.hashtag,
        include_btc=not args.no_btc,
        preview=args.preview,
        batch_size=args.batch_size,
    )

    if result["success"]:
        print("\n" + "=" * 60)
        print("âœ… ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
        print("=" * 60)
        print(f"ç·ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {result['total_tweets']} ä»¶")
        print(f"å¯¾è±¡ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: {result['filtered_count']} ä»¶")
        if not result.get("preview"):
            print(f"Pineconeã«ä¿å­˜: {result['imported_count']} ä»¶")
            print(f"DBå†…åˆè¨ˆ: {result.get('total_in_db', 'N/A')} ä»¶")
            if "date_range" in result:
                print(f"æœŸé–“: {result['date_range']['oldest']} ï½ {result['date_range']['newest']}")
        print("=" * 60)
    else:
        print(f"\nâŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {result.get('error', 'Unknown error')}")
        sys.exit(1)


if __name__ == "__main__":
    main()
